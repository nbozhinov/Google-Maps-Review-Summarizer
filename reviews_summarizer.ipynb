{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function for repeating http requests until they succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def request_data_from_url(url):\n",
    "    \"\"\"A utility function for making http requests and repeating them until they succeed \n",
    "    Args:\n",
    "        url (str): url for which to make a request\n",
    "    Returns: \n",
    "        http.client.HTTPResponse: an object which can work as a context manager and has at least the properties url, headers, and status.\n",
    "    \"\"\"\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try: \n",
    "            response = urllib.request.urlopen(url)\n",
    "            \n",
    "            # 200 is the HTTP response code for success\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(1)\n",
    "            print(\"Error for URL %s: %s\" % (url, datetime.datetime.now()))\n",
    "            print(\"Retrying...\")\n",
    "\n",
    "    return response.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function which given geographic coordinates and type of place fetches a list of places near those coordinates (in a radius of 1000 meters) and for each place it fetches its rating and first 5 reviews (the API returns just 5 by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def scrape_google_maps_data(api_key, location, place_type):\n",
    "    \"\"\"A utility function for getting a list of places and corresponding reviews with given type aroung given coordinates\n",
    "    Args:\n",
    "        api_key (str): an authentication key for the API of Google Maps\n",
    "        location (str): the geographical coordinates around which to search for places\n",
    "        place_type (str): the type of places to search for\n",
    "    Returns:\n",
    "        dictionary: the keys are the names of the places and the values are pairs of their numeric rating (number of stars) in Google Maps and a list containing the text of some of their reviews\n",
    "    \"\"\"\n",
    "    url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json?'\n",
    "    location_param = 'location=%s&' % (location)\n",
    "    radius_param = 'radius=1000&'\n",
    "    type_param = 'type=%s&' % (place_type)\n",
    "    key = 'key=%s' % (api_key)\n",
    "    \n",
    "    request_url = url + location_param + radius_param + type_param + key\n",
    "    \n",
    "    places_list = json.loads(request_data_from_url(request_url))\n",
    "    \n",
    "    result = {} \n",
    "    for place in places_list['results']:\n",
    "        details_url = 'https://maps.googleapis.com/maps/api/place/details/json?'\n",
    "        place_name = place['name'];\n",
    "        place_id_param = 'place_id=%s&' % (place['place_id'])\n",
    "        language_param = 'language=en&'\n",
    "        fields_param = 'fields=rating,reviews&'\n",
    "        details_request_url = details_url + place_id_param + language_param + fields_param + key\n",
    "        place_details = json.loads(request_data_from_url(details_request_url))\n",
    "        result[place['name']] = (place_details['result']['rating'], [review['text'] for review in place_details['result']['reviews']])\n",
    "    \n",
    "    #print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained models provided by spaCy, especially the small ones, don't recognized many foods' category of products, but just as nouns. So we need to extend the model with new examples. The next cells contains code and example data for retraining a preloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and procedure for retraining the model to recognise additional product types.\n",
    "TRAIN_DATA = [\n",
    "    (\"I love pizza\", {\"entities\": [(7, 12, \"PRODUCT\")]}),\n",
    "    (\"Let's get some ice cream.\", {\"entities\": [(15, 24, \"PRODUCT\")]}),\n",
    "    (\"Let's go to the restaurant.\", {\"entities\": [(16, 26, \"PRODUCT\")]}),\n",
    "    (\"Good food.\", {\"entities\": [(5, 9, \"PRODUCT\")]}),\n",
    "    (\"drinking\", {\"entities\": [(0, 8, \"PRODUCT\")]}),\n",
    "    (\"Good food.\", {\"entities\": [(5, 9, \"PRODUCT\")]}),\n",
    "]\n",
    "\n",
    "def train(nlp, n_iter=100):\n",
    "    \"\"\"A function which retrains the given model to recognize more words for food as belonging to the PRODUCT category.\n",
    "    Args:\n",
    "        nlp (Language object): existing spaCy pipeline with a loaded pretrained model which sholed be trainned additionaly\n",
    "        n_iter (int): number of itterations for retraining\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"\n",
    "    # Creates and adds the pipeline component necessary for the training\n",
    "    # nlp.create_pipe works only for built-in components of spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        # Just it them if it already exists\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # Add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # Disable pipelines, which are not necessary for the training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        examples = []\n",
    "        for text, annots in TRAIN_DATA:\n",
    "            examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(examples)\n",
    "            losses = {}\n",
    "            # pack the examples using spaCy minibatch\n",
    "            for batch in minibatch(examples, size=compounding(4.0, 32.0, 1.001)):\n",
    "                nlp.update(\n",
    "                    batch,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "#            print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains a list of superlatives which we will use for the scoring and their corresponding scores as well as some sample reviews for testing without the Google Maps API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Superlatives used for scoring:\n",
    "scores = {\n",
    "    \"terrible\" : -3,\n",
    "    \"awful\" : -2,\n",
    "    \"bad\" : -1,\n",
    "    \"reasonable\" : 0,\n",
    "    \"average\" : 0,\n",
    "    \"good\" : 1,\n",
    "    \"excellent\" : 2,\n",
    "    \"delicious\" : 2,\n",
    "    \"wonderful\" : 2,\n",
    "    \"perfect\" : 3,\n",
    "    \"superb\" : 3\n",
    "}\n",
    "# Extracted sample reviews for testing without Google Maps API key:\n",
    "example_reviews = {\n",
    "    \"Beykoz\":\n",
    "        (4.4, [\n",
    "            \"Good range of food and drinks from pizza, to kebabs, wine, beer and ice cream.\",\n",
    "            \"Amazing choice of wonderful food from anywhere you can imagine. Lots of seating, mostly sharing tables. Great atmosphere. Shop too, freshly baked bread, croissants, and all kinds of veg and other produce. Toilets clean and adequate but not best I've seen. I'll definitely go back if in the area.\",\n",
    "            \"Amongst the best food courts in London. Super spacious and a huge variety of delicious food and drink options across the site.\",\n",
    "            \"Really great atmosphere and vibes, lots of seats and food stalls with diverse cuisine to choose from! Great place for a big group, or a fun date.\",\n",
    "            \"Ok so. Let's review Beykoz. Probably you hear the name Beykoz, but this restaurant is not that one. Near Mogan lake, this restaurant is above average which come to mind with excellent service and steaks. In fact Beykoz is not only a restaurant that you can find good steak but also excellent drinking.\",\n",
    "            \"Excellent steaks! Good prices.. The ambience is perfect for a friend's night out!!\"]\n",
    "        )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function which given some text extracts pairs of foods and the adjectives which descibe them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_food_relations(doc):\n",
    "    \"\"\"A function which given some text extracts pairs of foods and the adjectives which descibe them directly.\n",
    "    Args:\n",
    "        doc (a spaCy Doc objec): the result of running dependency parsing with spaCy on some text\n",
    "    Returns: \n",
    "        list: a list of pairs (adjective, noun) where the noun is a type of product аnd the adjective is describing it\n",
    "    \"\"\"\n",
    "    # Merge entities и noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    # Remove overlaping phrases    \n",
    "    spans = spacy.util.filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in spans:\n",
    "            retokenizer.merge(span)\n",
    "\n",
    "    relations = []\n",
    "    for food in filter(lambda w: w.ent_type_ == \"PRODUCT\", doc):\n",
    "        if food.dep_ in {\"pobj\", \"ROOT\"} and food.head.dep_ in {\"adj\", \"prep\", \"ROOT\"}:\n",
    "            relations.append((food.head.head, food))\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function for extracting a numeric score form the review text. It consumes the dictionary which is returned by the function scrape_google_maps_data()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(places, model=\"en_core_web_sm\"):\n",
    "    \"\"\"The main function for extracting a numeric score form the review text. It consumes the dictionary which is returned by the function scrape_google_maps_data().\n",
    "    Args:\n",
    "        places (dictionary): a dictionary with object and their reviews fetched from the Google Maps API by using scrape_google_maps_data()\n",
    "        model (str): the name of the model to be loaded into spaCy\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(model)\n",
    "    print(\"The model '%s' is loaded into spaCy.\" % model)\n",
    "    train(nlp)\n",
    "    print(\"The model '%s' is retrained.\" % model)\n",
    "    print(\"Processing %d places.\" % len(places))\n",
    "\n",
    "    for name, reviews in places.items():\n",
    "        calculated_score = 0\n",
    "        num_superlatives = 0\n",
    "        sum_scores = 0\n",
    "        for text in reviews[1]:\n",
    "            doc = nlp(text)\n",
    "            relations = extract_food_relations(doc)\n",
    "            for r1, r2 in relations:\n",
    "                for word, score in scores.items():\n",
    "                    if r1.text.lower().find(word) != -1:\n",
    "                        sum_scores += score\n",
    "                        num_superlatives += 1\n",
    "        if num_superlatives != 0:\n",
    "            calculated_score = sum_scores / num_superlatives\n",
    "        print(\"place: %s - Google Maps raiting %s, calculated raiting %s\" % (name, reviews[0], calculated_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Google Maps API requires setting up and account and generating a special key, which is to be included in each request. Enabling the key requires adding a credit card for billing (even when staing within the free quota).\n",
    "The key included here is no longer active, so for testing there is a commented-out call using some hard-coded data located in one of the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyBsjuNsm0h04JJ_d7i3I1gYsMsYcslzQf8'\n",
    "location = '51.50732630711219,-0.12773362936048624'\n",
    "place_type = 'restaurant'\n",
    "\n",
    "places = scrape_google_maps_data(api_key, location, place_type)\n",
    "calculate_scores(places)\n",
    "# calculate_scores(example_reviews)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}